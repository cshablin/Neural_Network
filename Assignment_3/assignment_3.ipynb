{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_3_5_5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IytrQ6yvX0x4"
      },
      "source": [
        "import numpy as np\n",
        "from keras import callbacks\n",
        "from keras.callbacks import History\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import utils\n",
        "import string\n",
        "from typing import List, Tuple\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdcFOmChYPtm"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "\n",
        "def load_lyrics(csv_path: str) -> DataFrame:\n",
        "    df = pd.read_csv(csv_path, sep='\\n', header=None)\n",
        "    res = df.iloc[:, 0].str.rstrip(r'&, ').str.extract(r'([^,]+),([^,]+),(.+)')\n",
        "    res.columns = ['artist', 'title', 'lyrics']\n",
        "    return res\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdQYtAOFYchM"
      },
      "source": [
        "def create_x_y_train(songs, tokenize, total_words) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    input_sequences = []\n",
        "    for line in songs:\n",
        "        token_list = tokenize.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i + 1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    print(input_sequences[:10])\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=100, padding='pre'))\n",
        "    x_train = input_sequences[:, :-1]\n",
        "    y_train = input_sequences[:, -1]\n",
        "    y_train = utils.to_categorical(y_train, num_classes=total_words)\n",
        "    return x_train, y_train, max_sequence_len\n",
        "\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "\n",
        "def load_lyrics(csv_path: str) -> DataFrame:\n",
        "    df = pd.read_csv(csv_path, sep='\\n', header=None)\n",
        "    res = df.iloc[:, 0].str.rstrip(r'&, ').str.extract(r'([^,]+),([^,]+),(.+)')\n",
        "    res.columns = ['artist', 'title', 'lyrics']\n",
        "    return res\n",
        "\n",
        "def load_songs(path) -> List[str]:\n",
        "    df = load_lyrics(path)\n",
        "    songs = []\n",
        "    for song in list(df['lyrics']):\n",
        "        song += \" EOF\"\n",
        "        # remove '(*)'\n",
        "        modified_song = re.sub(r\"\\([^()]*\\)\", \"\", song)\n",
        "        modified_song = modified_song.replace(\"chorus\", \"\").lower()\n",
        "        # modified_song = modified_song.replace(\"chorus\", \"\").replace(\"&\", \"silencio\").lower()\n",
        "        # modified_song = modified_song.replace(\"&\", \"silencio\").lower()\n",
        "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "        modified_song = regex.sub('', modified_song)\n",
        "        songs.append(modified_song)\n",
        "    songs.pop(305)\n",
        "    return songs\n",
        "\n",
        "def remove_words(words: List[str], songs):\n",
        "    resulting_songs = []\n",
        "    for song in songs:\n",
        "        result = song\n",
        "        for word in words:\n",
        "            result = result.replace(word, '')\n",
        "        resulting_songs.append(result)\n",
        "    return resulting_songs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvcE5N-OYGtA"
      },
      "source": [
        "class LyricsGenerator(object):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, vocab_size: int, input_size: int, embedding_matrix: np.ndarray):\n",
        "        embedding_layer = Embedding(\n",
        "            vocab_size,\n",
        "            embedding_dim,\n",
        "            input_length=input_size,\n",
        "            weights=[embedding_matrix],\n",
        "            # embeddings_initializer=initializers.Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "\n",
        "        self.model = Sequential()\n",
        "        self.model.add(embedding_layer)\n",
        "        self.model.add(LSTM(units=embedding_dim))\n",
        "        self.model.add(Dropout(0.2))\n",
        "        self.model.add(Dense(units=vocab_size, activation='softmax'))\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "        self.model.summary()\n",
        "\n",
        "    def fit(self, x, y, hyper_parameters):\n",
        "        callback = callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "        callback2 = callbacks.LearningRateScheduler(self._lr_scheduler)\n",
        "        # callback3 = callbacks.ModelCheckpoint('lyrics_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "        # callback = callbacks.EarlyStopping(monitor='val_accuracy', mode='max', min_delta=1)\n",
        "        # history = self.model.fit(x, y, epochs=100, verbose=1)\n",
        "        history = self.model.fit(x, y, batch_size=hyper_parameters['batch_size'], epochs=hyper_parameters['epochs'],\n",
        "                                 callbacks=[callback, callback2],\n",
        "                                 verbose=1, validation_split=hyper_parameters['validation_split'],\n",
        "                                 validation_data=None)\n",
        "        self.model.save('lyrics_model.h5')\n",
        "        \n",
        "        return history\n",
        "\n",
        "    def _lr_scheduler(self, epoch, lr):\n",
        "        return 0.99 * lr\n",
        "\n",
        "    # to get a picture of loss progress.\n",
        "    def plot_metric(self, history: History, metric: str = 'loss') -> None:\n",
        "        import matplotlib.pyplot as plt\n",
        "        train_metrics = history.history[metric]\n",
        "        val_metrics = history.history['val_'+metric]\n",
        "        epochs = range(1, len(train_metrics) + 1)\n",
        "        plt.plot(epochs, train_metrics)\n",
        "        plt.plot(epochs, val_metrics)\n",
        "        plt.title('Training and validation '+ metric)\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(metric)\n",
        "        plt.legend([\"train_\"+metric, 'val_'+metric])\n",
        "        plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TouTqfYeaKTl",
        "outputId": "8dbe57a5-e91f-4df0-dd36-cdcd71d98456"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqwTVl0Wo5gV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6e4598-2580-4e4a-cf9f-bf7cf6101bf6"
      },
      "source": [
        "embedding_matrixs =  np.load(\"/content/drive/MyDrive/DL/Assignment_3/Lyrics/embedding_matrix_glove.npy\")\n",
        "embedding_matrixs.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7527, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg17AmZ0ZhL1"
      },
      "source": [
        "songs = load_songs('/content/drive/MyDrive/DL/Assignment_3/Lyrics/lyrics_train_set.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "tokenize = Tokenizer()\n",
        "tokenize.fit_on_texts(songs)\n",
        "total_words = len(tokenize.word_index) + 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdnjwhQazWkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d102390a-8af0-4069-bd8c-12cb866a0b44"
      },
      "source": [
        "# import tensorflow as tf\n",
        "# import random\n",
        "# tf.random.set_seed(1)\n",
        "# random.seed(2)\n",
        "# np.random.seed(3)\n",
        "import json\n",
        "non_words_dict = json.load(open('/content/drive/MyDrive/DL/Assignment_3/Lyrics/non_words.json', \"r\"))\n",
        "non_words_list = non_words_dict[\"non existing words\"]\n",
        "cleaned_songs = remove_words(non_words_list, songs)\n",
        "print(len(cleaned_songs))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Av0w6spUQE",
        "outputId": "39ca3a13-d031-422a-c015-e4d721d99296"
      },
      "source": [
        "# # Data preprocessing\n",
        "# tokenize = Tokenizer()\n",
        "# tokenize.fit_on_texts(cleaned_songs)\n",
        "# total_words = len(tokenize.word_index) + 1\n",
        "\n",
        "embedding_dim = 300\n",
        "x_train, y_train, max_sequence_len = create_x_y_train(cleaned_songs, tokenize, total_words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[221, 2451], [221, 2451, 2452], [221, 2451, 2452, 314], [221, 2451, 2452, 314, 3], [221, 2451, 2452, 314, 3, 47], [221, 2451, 2452, 314, 3, 47, 212], [221, 2451, 2452, 314, 3, 47, 212, 1], [221, 2451, 2452, 314, 3, 47, 212, 1, 67], [221, 2451, 2452, 314, 3, 47, 212, 1, 67, 18], [221, 2451, 2452, 314, 3, 47, 212, 1, 67, 18, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVe2ZUvKqFbl",
        "outputId": "335a8878-0ffb-496d-d9c8-9e26c9530e64"
      },
      "source": [
        "# Train model\n",
        "\n",
        "parameters = {\n",
        "    'batch_size' : 4 ,\n",
        "    'validation_split' : None ,\n",
        "    'epochs' : 4 ,\n",
        "    'val_data' : None\n",
        "}\n",
        "lyrics_generator = LyricsGenerator(embedding_dim, total_words, x_train.shape[1], embedding_matrixs)\n",
        "h = lyrics_generator.fit(x_train, y_train, parameters)\n",
        "# lyrics_generator.plot_metric(h)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 99, 300)           2258100   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 300)               721200    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 7527)              2265627   \n",
            "=================================================================\n",
            "Total params: 5,244,927\n",
            "Trainable params: 2,986,827\n",
            "Non-trainable params: 2,258,100\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "38234/38234 [==============================] - 289s 7ms/step - loss: 5.9240\n",
            "Epoch 2/4\n",
            "38234/38234 [==============================] - 287s 8ms/step - loss: 4.9248\n",
            "Epoch 3/4\n",
            "38234/38234 [==============================] - 283s 7ms/step - loss: 4.5882\n",
            "Epoch 4/4\n",
            "35938/38234 [===========================>..] - ETA: 16s - loss: 4.3468"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H7eN736nuUq"
      },
      "source": [
        "def generate_text(seed_text, eof, model, max_sequence_len, vocab_size):\n",
        "    next_word = \"\"\n",
        "    word_indices = np.arange(vocab_size) + 1\n",
        "    while next_word != eof:\n",
        "        token_list = tokenize.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted_proba = model.model.predict_proba(token_list, verbose=0)\n",
        "        # print(predicted_proba.reshape((predicted_proba.shape[1])).shape)\n",
        "        # sample the word index acording to predicted proba\n",
        "        chosen_index = np.random.choice(word_indices ,1 ,replace=False, p=predicted_proba.reshape((predicted_proba.shape[1])))\n",
        "\n",
        "        \n",
        "        next_word = \"\"\n",
        "        for word, index in tokenize.word_index.items():\n",
        "            if index == chosen_index:     \n",
        "                next_word = word\n",
        "                print(next_word)\n",
        "                break\n",
        "        seed_text += \" \" + next_word\n",
        "    return seed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMitghcqkXWj"
      },
      "source": [
        "print(generate_text('forgive' , \"eof\" , lyrics_generator , max_sequence_len, total_words))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}