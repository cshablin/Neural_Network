{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IytrQ6yvX0x4"
      },
      "source": [
        "import numpy as np\n",
        "from keras import callbacks\n",
        "from keras.callbacks import History\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import utils\n",
        "import string\n",
        "from typing import List, Tuple\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdcFOmChYPtm"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "\n",
        "def load_lyrics(csv_path: str) -> DataFrame:\n",
        "    df = pd.read_csv(csv_path, sep='\\n', header=None)\n",
        "    res = df.iloc[:, 0].str.rstrip(r'&, ').str.extract(r'([^,]+),([^,]+),(.+)')\n",
        "    res.columns = ['artist', 'title', 'lyrics']\n",
        "    return res\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdQYtAOFYchM"
      },
      "source": [
        "def create_x_y_train(songs, tokenize, total_words) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    input_sequences = []\n",
        "    for line in songs:\n",
        "        token_list = tokenize.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i + 1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    print(input_sequences[:10])\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "    x_train = input_sequences[:, :-1]\n",
        "    y_train = input_sequences[:, -1]\n",
        "    y_train = utils.to_categorical(y_train, num_classes=total_words)\n",
        "    return x_train, y_train\n",
        "\n",
        "\n",
        "def load_songs(path) -> List[str]:\n",
        "    df = load_lyrics(path)\n",
        "    songs = []\n",
        "    for song in list(df['lyrics']):\n",
        "        song += \" EOF\"\n",
        "        modified_song = re.sub(r\"\\([^()]*\\)\", \"\", song)\n",
        "        # modified_song = re.sub(r\"\\[[^()]*\\]\", \"\", modified_song)\n",
        "        # for ch in [\"[\", \"]\", \"chorus\"]:\n",
        "        #     modified_song = modified_song.replace(ch, \"\")\n",
        "        modified_song = modified_song.replace(\"chorus\", \"\").replace(\"&\", \"silencio\").lower()\n",
        "        # modified_song = modified_song.replace(\"&\", \"silencio\").lower()\n",
        "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "        modified_song = regex.sub('', modified_song)\n",
        "        songs.append(modified_song)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvcE5N-OYGtA"
      },
      "source": [
        "class LyricsGenerator(object):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, vocab_size: int, input_size: int, embedding_matrix: np.ndarray):\n",
        "        embedding_layer = Embedding(\n",
        "            vocab_size,\n",
        "            embedding_dim,\n",
        "            input_length=input_size,\n",
        "            weights=[embedding_matrix],\n",
        "            # embeddings_initializer=initializers.Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "\n",
        "        self.model = Sequential()\n",
        "        self.model.add(embedding_layer)\n",
        "        self.model.add(LSTM(units=embedding_dim))\n",
        "        self.model.add(Dropout(0.2))\n",
        "        self.model.add(Dense(units=vocab_size, activation='softmax'))\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "        self.model.summary()\n",
        "\n",
        "    def fit(self, x, y, hyper_parameters):\n",
        "        callback = callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "        callback2 = callbacks.LearningRateScheduler(self._lr_scheduler)\n",
        "        # callback3 = callbacks.ModelCheckpoint('lyrics_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "        # callback = callbacks.EarlyStopping(monitor='val_accuracy', mode='max', min_delta=1)\n",
        "        # history = self.model.fit(x, y, epochs=100, verbose=1)\n",
        "        history = self.model.fit(x, y, batch_size=hyper_parameters['batch_size'], epochs=hyper_parameters['epochs'],\n",
        "                                 callbacks=[callback, callback2],\n",
        "                                 verbose=1, validation_split=hyper_parameters['validation_split'],\n",
        "                                 validation_data=None)\n",
        "        self.model.save('lyrics_model.h5')\n",
        "        \n",
        "        return history\n",
        "\n",
        "    def _lr_scheduler(self, epoch, lr):\n",
        "        return 0.99 * lr\n",
        "\n",
        "    # to get a picture of loss progress.\n",
        "    def plot_metric(self, history: History, metric: str = 'loss') -> None:\n",
        "        import matplotlib.pyplot as plt\n",
        "        train_metrics = history.history[metric]\n",
        "        val_metrics = history.history['val_'+metric]\n",
        "        epochs = range(1, len(train_metrics) + 1)\n",
        "        plt.plot(epochs, train_metrics)\n",
        "        plt.plot(epochs, val_metrics)\n",
        "        plt.title('Training and validation '+ metric)\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(metric)\n",
        "        plt.legend([\"train_\"+metric, 'val_'+metric])\n",
        "        plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TouTqfYeaKTl",
        "outputId": "43255036-1306-4700-fa27-2dd0db43e38b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqwTVl0Wo5gV"
      },
      "source": [
        "embedding_matrixs =  np.load(\"/content/drive/MyDrive/DL/Assignment_3/Lyrics/embedding_matrix.npy\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg17AmZ0ZhL1",
        "outputId": "65a4d111-eeea-4713-825c-2ddd08f0eb6a"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/DL/Assignment_3/Lyrics/lyrics_train_set.csv', sep='\\n', header=None)\n",
        "res = df.iloc[:, 0].str.rstrip(r'&, ').str.extract(r'([^,]+),([^,]+),(.+)')\n",
        "print(len(res))\n",
        "res.columns = ['artist', 'title', 'lyrics']\n",
        "print(len(res['lyrics']))\n",
        "\n",
        "songs = []\n",
        "for song in list(res['lyrics']):\n",
        "    song += \" EOF\"\n",
        "    modified_song = re.sub(r\"\\([^()]*\\)\", \"\", song)\n",
        "    modified_song = modified_song.replace(\"chorus\", \"\").replace(\"&\", \"silencio\").lower()\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    modified_song = regex.sub('', modified_song)\n",
        "    songs.append(modified_song)\n",
        "\n",
        "len(songs)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "600\n",
            "600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdnjwhQazWkG"
      },
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "tf.random.set_seed(50)\n",
        "random.seed(50)\n",
        "np.random.seed(50)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Av0w6spUQE",
        "outputId": "bfd00223-16e7-44da-b2a0-f68fcee790c6"
      },
      "source": [
        "# Data preprocessing\n",
        "tokenize = Tokenizer()\n",
        "tokenize.fit_on_texts(songs)\n",
        "total_words = len(tokenize.word_index) + 1\n",
        "\n",
        "embedding_dim = 300\n",
        "x_train, y_train = create_x_y_train(songs, tokenize, total_words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[222, 2452], [222, 2452, 2453], [222, 2452, 2453, 1], [222, 2452, 2453, 1, 315], [222, 2452, 2453, 1, 315, 4], [222, 2452, 2453, 1, 315, 4, 48], [222, 2452, 2453, 1, 315, 4, 48, 213], [222, 2452, 2453, 1, 315, 4, 48, 213, 2], [222, 2452, 2453, 1, 315, 4, 48, 213, 2, 68], [222, 2452, 2453, 1, 315, 4, 48, 213, 2, 68, 19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVe2ZUvKqFbl",
        "outputId": "455f3580-dbdc-4544-dda0-ceb632893f95"
      },
      "source": [
        "# Train model\n",
        "\n",
        "parameters = {\n",
        "    'batch_size' : 4 ,\n",
        "    'validation_split' : None ,\n",
        "    'epochs' : 4 ,\n",
        "    'val_data' : None\n",
        "}\n",
        "lyrics_generator = LyricsGenerator(embedding_dim, total_words, x_train.shape[1], embedding_matrixs)\n",
        "h = lyrics_generator.fit(x_train, y_train, parameters)\n",
        "lyrics_generator.plot_metric(h)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1480, 300)         2262300   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 300)               721200    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 7541)              2269841   \n",
            "=================================================================\n",
            "Total params: 5,253,341\n",
            "Trainable params: 2,991,041\n",
            "Non-trainable params: 2,262,300\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            " 2638/44611 [>.............................] - ETA: 47:33 - loss: 6.4158"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H7eN736nuUq"
      },
      "source": [
        "def generate_text(seed_text, eof, model, max_sequence_len, vocab_size):\n",
        "    next_word = \"\"\n",
        "    resulting_text = \"\"\n",
        "    word_indices = np.arange(vocab_size) + 1\n",
        "    while output_wnext_wordord != eof:\n",
        "        token_list = tokenize.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted_proba = model.predict_proba(token_list, verbose=0)\n",
        "        # sample the word index acording to predicted proba\n",
        "        chosen_index = np.random.choice(word_indices ,1 ,replace=False, p=predicted_proba)\n",
        "\n",
        "        \n",
        "        next_word = \"\"\n",
        "        for word, index in tokenize.word_index.items():\n",
        "            if index == chosen_index:\n",
        "                next_word = word\n",
        "                resulting_text += \" \" + next_word\n",
        "                break\n",
        "    return resulting_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMitghcqkXWj"
      },
      "source": [
        "print(generate_text('forgive' , \"EOF\" , lyrics_generator , max_sequence_len, total_words))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}